{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Nuts &amp; Bolts of Machine Learning","text":"<p> Model: ChatGPT  Prompt: Create an image of Artificial Intelligence in space</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#sachin-umrao","title":"Sachin Umrao","text":"<p>After completing my master's degree in Mechanical Engineering,  I am currently working as a Senior Data Scientist at Fidelity Investments in Bengaluru. In my free time, I enjoy reading books, watching TV series, and exploring emerging technologies in the fields of Large Language Models (LLMs) and Reinforcement Learning. </p> <p>You can find me @ Twitter Reddit LinkedIn Github</p> <p>or just drop a mail @ sachin.umrao.1512@gmail.com</p> <p> Ananthagiri Hills, Aug 2019</p> <p> Nandi Hills, Jun 2024</p>"},{"location":"library/","title":"Library","text":"<p>\"One glance at a book and you hear the voice of another person, perhaps someone dead for 1,000 years. To read is to voyage through time.\" - Carl Sagan</p>"},{"location":"library/#machine-learning","title":"Machine Learning","text":"<ul> <li>Pattern Recognition and Machine Learning</li> <li>Machine Learning \u2013 A Probabilistic Perspective</li> <li>An Introduction to Statistical Learning</li> <li>Deep Learning</li> <li>Deep Learning Foundations and Concepts</li> <li>Understanding Deep Learning</li> <li>Reinforcement Learning: An Introduction</li> </ul>"},{"location":"library/#system-design","title":"System Design","text":"<ul> <li>System Design Interview Volume-1</li> <li>System Design Interview Volume-2</li> <li>Machine Learning System Design Interview</li> <li>Designing Machine Learning Systems</li> <li>Designing Distributed Systems</li> <li>Generative AI System Design Interview</li> </ul>"},{"location":"library/#programming","title":"Programming","text":"<ul> <li>The Algorithm Design Manual</li> <li>Data Structures and Algorithms in Python - Goodrich, Tamassia &amp; Goldwasser</li> <li>Elements of Programming in Python</li> <li>Python Tricks</li> <li>Effective python</li> <li>Python Distilled</li> <li>Programming in Scala</li> <li>Learning Ray</li> <li>The C++ Programming Langugae</li> <li>The C Programming Language</li> <li>Computer Systems: A Programmer's Perspective</li> </ul>"},{"location":"library/#software-development","title":"Software Development","text":"<ul> <li>Clean Code</li> <li>The Clean Coder</li> <li>The Pragmatic Programmer</li> <li>The Software Engineer\u2019s Guidebook</li> </ul>"},{"location":"library/#fiction","title":"Fiction","text":"<ul> <li>Children fo Time</li> <li>Children fo Ruin</li> <li>Children of Memory</li> <li>Ender\u2019s Game</li> <li>Wuthering Heights</li> <li>Pride and Prejudice</li> <li>The Great Gatsby</li> <li>The Picture of Dorian Gray</li> <li>The Namesake</li> <li>Hercule Poirot Stories Collection</li> <li>The Utlimate Hitchhiker's Guide to the Galaxy</li> <li>The Shinning</li> <li>Franz Kafka Selected Stories</li> <li>Siddhartha</li> </ul>"},{"location":"library/#non-fiction","title":"Non-Fiction","text":"<ul> <li>Thinking in Systems</li> <li>The Black Swan</li> <li>Antifragile</li> <li>Fooled by Randomness</li> <li>The Idea Factory</li> <li>The Innovators</li> <li>The Man from the Future</li> <li>Infinite Powers</li> <li>The Joy of X</li> <li>Euclid\u2019s Window</li> <li>On Writing Well</li> <li>Creative Confidence</li> <li>The Almanack of Naval Ravikant</li> <li>The Intelligent Investor</li> <li>Originals</li> </ul>"},{"location":"library/#wishlist","title":"Wishlist","text":"<ul> <li>Learning Go - Jon Bodner</li> <li>AI Engineering - Chip Huyen</li> <li>Black Holes - Brain Cox</li> <li>Our Mathematical Universe - Max Tegmark</li> <li>How to Solve it - G Polya</li> <li>A Mind at Play</li> <li>Cosmos - Carl Sagan</li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/","title":"Useful Python Libraries for Machine Learning","text":"<p> Image Credit: https://www.analyticsindiamag.com</p> <p>If you are a new to the field of machine learning, you can get overwhelmed  with the variety of sub-fields available under the umbrella of machine  learning. For each subfield, there are specific libraries to help you  with getting the task done. In this blog, we will glance over the  different libraries and their use cases.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#general-purpose-machine-learning-libraries","title":"General Purpose Machine Learning Libraries","text":"","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#scikit-learn","title":"Scikit-Learn","text":"<p>Scitkit-learn provides all popular machine learning algorithms and  functions for data transformation to error analysis pre-built into it.  It is a recommended library for getting started with machine learning.  One can tweak the parameters in algortihms and see the changes in the  performance of model.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#tensorflow","title":"Tensorflow","text":"<p>Most of the people take tensorflow as deep learning library but at its core  it just converts mathematical operations into a computational grpahs.  Hence, any machine learning algorithm can be developed in tensorlfow.  The implementations of Logistic Regression and Linear Regression in  tensorlfow can be easily found in web.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#working-with-videosimages","title":"Working with Videos/Images","text":"","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#opencv","title":"OpenCV","text":"<p>OpenCV is the most popular library for working with images and videos.  It provides all basic operations to be executed on images.  It enables us to control webcam and take images or video-feed from it.  Hence, it becomes suitable for live projects of object detection,  face recognition etc.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#scikit-image","title":"Scikit-Image","text":"<p>It provides optimized image transformation operations.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#dlib","title":"DLib","text":"<p>It provides deep learning based implementations for face detection and  object detection.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#working-with-text-data","title":"Working with Text Data","text":"","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#nltk","title":"NLTK","text":"<p>NLTK (Natural Language TooKit) provides basic operations for text  pre-processing.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#spacy","title":"Spacy","text":"<p>Spacy provides all text pre-processing functions along with deep  learning based models for various NLP tasks.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#stanford-nlp","title":"Stanford-NLP","text":"<p>It is developed in Stanford as evidnet from name. It is a cutting edge  library which features latest developments in the field of NLP.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#gensim","title":"Gensim","text":"<p>Gensim contains various trained models for word-embeddings and  topic-modelling.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#reinforcement-learning","title":"Reinforcement Learning","text":"","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#openai-gym","title":"OpenAI Gym","text":"<p>For most of reinforcement leraning tasks, an enviornment is required  where agent can perform actions and learn. Gym provides various  environments where one can train agents. The environment includes atari games,  robotic simulations and alot of other fancy things.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#pygame","title":"PyGame","text":"<p>If you want to create a new environment (or a new game) and train agent in  it, you can build it with PyGame.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#keras-rl","title":"Keras-RL","text":"<p>Keras-RL is deep reinforcement learning library. It provides deep  architectures which can be trained in Gym or other environment.  It features popular algorithms such as DQN.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#deep-learning-libraries","title":"Deep Learning Libraries","text":"","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#theano","title":"Theano","text":"<p>Theano was developed by MILA and it is predecessor of all deep  learning frameworks. But now MILA has stopped its development  after 1.0 release.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#tensorflow_1","title":"Tensorflow","text":"<p>Tensorflow is the most popular library for machine learning. It emphasized on the concept of computational graphs. It is well suited for distributed computing. It is supported by Google.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#pytorch","title":"Pytorch","text":"<p>Pytorch is being developed by Facebook and it is the main rival of  Tensorflow. Pytorch also provides a dynamic graph execution.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#keras","title":"Keras","text":"<p>Keras is a high-level wrapper on existing deep learning frameworks.  It supports theano, tensorflow and mxnet as its backends.  It is recommended for beginners as deep learning architectures  can be made very easily, quickly and without in-depth knowledge  of neural networks.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#other-deep-learning-libraries-","title":"Other Deep Learning Libraries:-","text":"","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#h2o","title":"H2O","text":"<p>Supported by H2O.ai which provide several deep learning solutions such as  Driverless AI</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#caffe","title":"Caffe","text":"<p>It was originally developed by Berkley AI Research (BAIR) group,  but now it is advanced by Facebook</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#cntk","title":"CNTK","text":"<p>A deep learning library provided by Microsift</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#mxnet","title":"MXNet","text":"<p>MXNet is supported by Amazon</p> <p>Most of the  deep learing libraries have two different versions for CPU  and GPU. The GPU version of the library takes advantage of CUDA  acceleration and performance gain is enormous. So, if you have Nvidia  graphics card, setup CUDA and install gpu version of the library.</p> <p></p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#other-supporting-libraries","title":"Other Supporting Libraries","text":"","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#numpy","title":"Numpy","text":"<p>Numpy is a library for processing numerical data. It is useful in data  pre-processing step. It gives good performance when vectorization is used.  It also provides very optimized implementations of common mathematical  operations.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#pandas","title":"Pandas","text":"<p>Pandas is used for mix data as it can handle strings and numerical data as  well. Several useful transformations are already implemented (such as fixing  null values, one-hot encoding) and easily handles datetime data.  It has become popular for data preprocessing.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#scipy","title":"Scipy","text":"<p>Scipy provides complex mathematical operations such as convolution and  matrix inversions along with various statistical functions. It also  serves as backend of Scikit-Learn and Scikit-Image.</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#matplotlib","title":"Matplotlib","text":"<p>Matplotlib is most popular library for data visualization.  Data visualization is helpful in analyzing the data during EDA  (Exploratory Data Analysis).</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/useful-python-libraries-for-machine-learning/#other-popular-libraries-for-data-visualization-","title":"Other popular libraries for data visualization:-","text":"<p>Bokeh, Plotly, Seaborn, Graphviz</p>","tags":["Python","Libraries"]},{"location":"blog/2019/08/15/keyword-extraction-methods/","title":"Keyword Extraction","text":"<p> Image Credit: www.adaringadventure.com</p>","tags":["NLP"]},{"location":"blog/2019/08/15/keyword-extraction-methods/#keyword-extraction-using-rake","title":"Keyword Extraction using RAKE","text":"","tags":["NLP"]},{"location":"blog/2019/08/15/keyword-extraction-methods/#1-why-perform-keyword-extraction","title":"1. Why perform keyword extraction?","text":"<ul> <li>Reduces the dimensionality of text </li> <li>Find summery of text (What this documents is about?)</li> <li>Fetch similar documents in Information Retrieval System</li> <li>Classify Documents when there are large number of categories</li> </ul>","tags":["NLP"]},{"location":"blog/2019/08/15/keyword-extraction-methods/#2-how-to-perform-keyword-extraction","title":"2. How to perform keyword extraction?","text":"<pre><code>I. Candidate Selection\n\nII. Property Calculation\n\nIII. Scoring of potential keywords and final selection\n</code></pre>","tags":["NLP"]},{"location":"blog/2019/08/15/keyword-extraction-methods/#3-rapid-automatic-keyword-extraction-rake","title":"3. Rapid Automatic Keyword Extraction (RAKE)","text":"<ul> <li> <p>Use punctuations and stopwords as boundaries. Keyword phrases are assumed to be lying between these boundaries (in ideal case tokens should be stemmed as well before this step). These words are often called Candidate Words/Phrases.</p> </li> <li> <p>RAKE computes the properties of each candidate, which is the sum of score for each word in phrase. The words are scored according to their frequency and typical length of candidate phrase in which they appear.</p> </li> <li> <p>Rank keywords based on RAKE score</p> </li> </ul>","tags":["NLP"]},{"location":"blog/2019/08/15/keyword-extraction-methods/#4-error-estimation","title":"4. Error Estimation","text":"<ul> <li> <p>Match keywords obtained from model with keywords hand assigned to the document.</p> </li> <li> <p>Precision : Percentage of correct keywords among those extracted</p> </li> <li> <p>Recal : Percentage of correctly extracted keywords among all correct ones</p> </li> </ul>","tags":["NLP"]},{"location":"blog/2019/08/15/keyword-extraction-methods/#additional-notes","title":"Additional Notes","text":"","tags":["NLP"]},{"location":"blog/2019/08/15/keyword-extraction-methods/#advantages-of-rake","title":"Advantages of RAKE","text":"<pre><code>I. Doamin Independence\n\nII. Good Precision\n</code></pre>","tags":["NLP"]},{"location":"blog/2019/08/15/keyword-extraction-methods/#scoring-in-rake","title":"Scoring in RAKE","text":"<p>Score(word) = Degree(word) / Frequency(word)</p> <p>Frequency(word) = Number of times word occurs in document</p> <p>Degree(word) = Similar to degree of node in a graph. It is number of times a certain word co-occurs with other candidate keyword </p>","tags":["NLP"]},{"location":"blog/2024/12/25/top-llm-papers-of-2024/","title":"Top LLM Papers of 2024","text":"Qwen2 Technical Report Qwen2.5 Technical Report Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence Gemma: Open Models Based on Gemini Research and Technology Gemma 2: Improving Open Language Models at a Practical Size The Llama 3 Herd of Models Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference Large Language Models: A Survey","tags":["LLMs","Research_Papers"]},{"location":"blog/2024/02/15/free-machine-learning-courses/","title":"Free Machine Learning and AI Courses","text":"","tags":["Machine_Learning","Deep_Learning","Learning_Resource","CS","Maths"]},{"location":"blog/2024/02/15/free-machine-learning-courses/#machine-learning-deep-learning","title":"Machine Learning / Deep Learning","text":"<ul> <li>CS229 Machine Learning</li> <li>CS229M Machine Learning Theory</li> <li>Cornell CS5787 Applied ML</li> <li>CS230 Deep Learning</li> <li>CMU Deep Learning</li> <li>NYU Deep Learning</li> <li>CS 294 Deep Unsupervised Learning</li> </ul>","tags":["Machine_Learning","Deep_Learning","Learning_Resource","CS","Maths"]},{"location":"blog/2024/02/15/free-machine-learning-courses/#deep-learning-for-computer-vision","title":"Deep Learning for Computer Vision","text":"<ul> <li>CS231N Deep Learing for Computer Vision</li> </ul>","tags":["Machine_Learning","Deep_Learning","Learning_Resource","CS","Maths"]},{"location":"blog/2024/02/15/free-machine-learning-courses/#natural-language-processing","title":"Natural Language Processing","text":"<ul> <li>CS124 From Language to Information</li> <li>CS224N Natural Language Processing with Deep Learning</li> <li>CS224U Natural Language Understanding</li> <li>CS25 Transformers United</li> <li>CMU CS11-711 Advanced Natural Language Processing</li> <li>UMASS CS685 Advanced Natural Language Processing</li> </ul>","tags":["Machine_Learning","Deep_Learning","Learning_Resource","CS","Maths"]},{"location":"blog/2024/02/15/free-machine-learning-courses/#graph-machine-learning","title":"Graph Machine Learning","text":"<ul> <li>CS224W Machine Learning with Graphs</li> </ul>","tags":["Machine_Learning","Deep_Learning","Learning_Resource","CS","Maths"]},{"location":"blog/2024/02/15/free-machine-learning-courses/#reinforcement-learning","title":"Reinforcement Learning","text":"<ul> <li>CS234 Reinforcement Learning</li> <li>CS224R Deep Reinforcement Learning</li> <li>CS330 Deep Mutli-Task and Meta Learning</li> <li>CS285 Deep RL</li> <li>DeepMind UCL RL Lecture Series</li> <li>DeepMind RL Lectures - David Silver</li> <li>Deep RL Bootcamp</li> </ul>","tags":["Machine_Learning","Deep_Learning","Learning_Resource","CS","Maths"]},{"location":"blog/2024/02/15/free-machine-learning-courses/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>CS221 Artificial Intelligence: Principles and Techniques</li> <li>CS228 Probabilistic Graphical Models</li> <li>CS329S Machine Learning Systems Design</li> <li>Tiny ML and Efficient Deep Leanring Computing</li> </ul>","tags":["Machine_Learning","Deep_Learning","Learning_Resource","CS","Maths"]},{"location":"blog/2024/02/15/free-machine-learning-courses/#mathematics-for-machine-learning","title":"Mathematics for Machine Learning","text":"<ul> <li>Linear Algebra</li> <li>Probability and Statistics</li> <li>Essence of Calculus</li> <li>Essence of Linear Algebra</li> </ul>","tags":["Machine_Learning","Deep_Learning","Learning_Resource","CS","Maths"]},{"location":"blog/2024/02/15/free-machine-learning-courses/#basic-computer-science","title":"Basic Computer Science","text":"<ul> <li>Missing Semester</li> <li>Computer Systems</li> <li>Operating Systems</li> <li>Distributed Systems</li> <li>Database Systems</li> </ul>","tags":["Machine_Learning","Deep_Learning","Learning_Resource","CS","Maths"]},{"location":"blog/2024/02/15/phases-of-a-machine-learning-project/","title":"Phases of a Machine Learning Project","text":"Phase Useful Tools/Libraries Data Acquisition SQL, pandas Exploratory Data Analysis numpy, pandas, duckdb Data Annotation doccano Data Visualization matplotlib, seaborn, plotly Data Cleaning / Featurization numpy, pandas Model Development scikit-learn, xgboost, lightgbm, pytorch, tensorflow, wandb (experiment tracking) Error Analysis scikit-learn, matplotlib Hyperparameter Tuning optuna, ray Model Evaluation scikit-learn, matplotlib Model Explainability shap, lime Model Demo streamtlit, gradio","tags":["Python","Libraries","Machine_Learning"]},{"location":"blog/2025/02/02/brief-summary-of-qwen-models/","title":"Brief Summary of Qwen2 and Qwen 2.5 Models","text":"","tags":["NLP","LLM","Research_Papers"]},{"location":"blog/2025/02/02/brief-summary-of-qwen-models/#introduction-qwen2","title":"Introduction - Qwen2","text":"<p>Qwen Team is part of Alibaba Group based in China. Qwen2 Technical Report was released in Sep 2024 along with Qwen2 series of models. Qwen2 models are Large Langue Models (LLMs) trained using the next-token prediction and mostly follows the decoder only transformer architecture. This series include 4 dense models (all parameters acre activated during inference) of sizes 0.5B, 1.5B, 7B and 72B along with a Mixture of Experts (MoE) model of 57B parameters, of which 14B are activated for each token during inference.</p> <p>Models were trained on large scale dataset containing &amp; trillion tokens. It also included enhanced quality and quantity fo code and mathematics content. Authors hypothesize that enhancement in data led to improved reasoning abilities of the models. </p> <p>For post-training, models underwent Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to align them with human preference and learn form human feedback.</p>","tags":["NLP","LLM","Research_Papers"]},{"location":"blog/2025/02/02/brief-summary-of-qwen-models/#tokenizer-and-model-details","title":"Tokenizer and Model details","text":"<p>Tokenizer used is based on byte-level byte-pair encoding. Vocabulary consists of 151643 regular tokens and 3 control tokens.</p> <p>Key differences in the Qwen model architecture are following:</p> <ul> <li> <p>Groped Query Attention Model uses Grouped Query Attention (GQA) instead of usual Multi-Headed Attention. GQA optimizes QV cache during inference.</p> </li> <li> <p>Dual Chunk Attention (DCA) DCA enables model to break long sequences into chunks. It helps in increasing the context length that model can handle. If input fits within a chunk, DCA produces same output as self-attention. If input spans multiple chunks, DCA captures relative positional information between tokens within and across chunks.</p> </li> <li> <p>YARN Qwen2 models use YARN to rescale attention weights for better length extrapolation</p> </li> <li> <p>Models use SwiGLU activation, RoPE for positional embedding, QKV bias for attention and RMSNorm plus pre-normalization to stabilize model training.</p> </li> </ul>","tags":["NLP","LLM","Research_Papers"]},{"location":"blog/2025/02/02/brief-summary-of-qwen-models/#pre-training","title":"Pre-Training","text":"<ul> <li>Heuristics based as well as LLM based methods utilizing prior version of Qwen models was used to filter out low-quality data.</li> <li>LLMs were also used to synthesize high-quality pre-training data.</li> <li>Pre-training data was expanded form 3 trillion tokens in Qwen1.5 model to 7 trillion. It included high quality code, mathematics and multilingual datasets.</li> <li>High quality multi-task instruction data was utilized in pre-training stage to enhance in-context learning and instruction-following abilities.</li> <li>Context length of 4096 to 32,768 tokens was during pre-training.</li> <li>With help from DCA and YARN, models can effectively handles sequences up to 131,072 tokens.</li> <li>Pre-training was performed using Causal Language Modeling (next word prediction) objective.</li> </ul>","tags":["NLP","LLM","Research_Papers"]},{"location":"blog/2025/02/02/brief-summary-of-qwen-models/#post-training","title":"Post-Training","text":"<ul> <li>Post-training ensures model's output is aligned with human values and human-preferences.</li> <li>Post-training focuses on scalable alignment with minimal human annotation.</li> <li> <p>Post-training data consists of two parts:</p> <ul> <li>Demonstration Data for SFT: D = {(x<sub>i</sub>, y<sub>i</sub>)}</li> <li>Preference Data for RLHF: P = {(x<sub>i</sub>, y<sub>i</sub><sup>+</sup>,  y<sub>i</sub><sup>-</sup>)}</li> </ul> </li> <li> <p>Demonstration data consists of instruction and response pairs.</p> </li> <li> <p>Preference data includes instruction and two responses where one response is preferred over the other.</p> </li> <li> <p>Post-training data was built in two stages:</p> <ol> <li>Collaborative Annotation</li> <li>Automated Data Synthesis</li> </ol> </li> <li> <p>For collaborative annotation, authors first generated diverse, high-quality instructions from large instruction corpus. Then through human annotation, response y<sub>i</sub> and preference responses  y<sub>i</sub><sup>+</sup> and  y<sub>i</sub><sup>-</sup> are generated.</p> </li> <li> <p>By prompting Qwen models, authors added constraints/requirements to existing instructions to increase their complexity and add diverse difficulty levels.</p> </li> <li> <p>By prompting Qwen model, authors generated multiple responses to a given instruction using diverse generation strategies, which then were annotated by human annotator for preference ranking.</p> </li> <li> <p>To increase the size of instruction dataset, authors implemented multiple strategies to automatically synthesize high quality data. </p> <ul> <li>Rejection Sampling: LLMs generate multiple responses for a given instruction. Responses considered reasonable by LLM model are kept. Preference data is gathered using contrasting responses.</li> <li>Execution Feedback: For coding tasks, LLMs generate solution and test cases for a given instruction. Then solution is compiled and executed against test cases. </li> <li>Data Repurposing: High quality literature is given to LLMs to generate instructions and responses of various level of details.</li> <li>Constitutional Feedback: Constitutional AI refers to the process of guiding LLMs to generate responses based on predefined set of principles. To ensure model aligns with human values, a constitution dataset was built. Dataset includes what principles to follow and what to avoid. This dataset was used for creating responses which follow and deviate from the constitution and compiled into preference dataset. </li> </ul> </li> <li> <p>Using these methods, authors have amassed a large instruction dataset of ~500K samples, covering skills like instruction following, coding, mathematics, logical reasoning, role-playing, multilingualism and safety.</p> </li> <li> <p>Model was fine-tuned for two epochs with sequence length of 32,768 tokens. Learning rate was gradually reduced from 7 x 10<sup>-6</sup> to 7 x 10<sup>-7</sup>. To avoid overfitting weight decay was set to 0.1 and gradients were clipped at maximum value of 1.0.</p> </li> <li> <p>For RLHF training, model used two stages:</p> <ul> <li>Offline Stage: Model was fine-tuned on offline preference dataset using DPO.</li> <li>Online Stage: Model was fine-tuned using online reward model for immediate feedback. Model generated multiple responses by sampling current policy and reward model selected most and least preferred responses forming preference pairs using by DPO for each episode. Model used Online Merging Optimizer to avoid reduction in model performance due to alignment aka alignment tax.</li> </ul> </li> </ul>","tags":["NLP","LLM","Research_Papers"]},{"location":"blog/2025/02/02/brief-summary-of-qwen-models/#evaluation-summary","title":"Evaluation Summary","text":"<p>For detailed comparsion across benchmarks and other model, please refer to the technical report linked in references.</p> <ul> <li>Qwen2 models show improvements over prior model versions in both core LLM capabilities as well as instruction following abilities.</li> <li>Data scaling and high quality data during pre-training stage helps in enhancing model performance for models across parameter sizes.</li> </ul>","tags":["NLP","LLM","Research_Papers"]},{"location":"blog/2025/02/02/brief-summary-of-qwen-models/#qwen25-models-what-changed-from-qwen2","title":"Qwen2.5 Models - What changed from Qwen2?","text":"<p>Loading...</p>","tags":["NLP","LLM","Research_Papers"]},{"location":"blog/2025/02/02/brief-summary-of-qwen-models/#reference","title":"Reference","text":"<ul> <li>Qwen2 Technical Report</li> <li>Qwen2.5 Technical Report</li> </ul>","tags":["NLP","LLM","Research_Papers"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/","title":"Setting Up Ubuntu/Debian System for Deep Learning","text":"<p> Image Credit: https://mytechshout.com</p> <p>As I started my journey to learn coding, I was introduced to Linux by my mentors.  Primarily Linux was used by programmers but now a days it has garnered  popularity in all domains. There are many Linux based operating  systems  available out there but Ubuntu has become most used UNIX based OS.  It is widely used in industries and universities. In this blog we will  discuss what applications and tools are required/suggested to setup in a  fresh ubuntu installation for working on machine learning and deep  learning projects.</p>","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#update-the-system","title":"Update the System","text":"<pre><code>$ sudo apt update\n$ sudo apt upgrade\n</code></pre>","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#basic-utilities","title":"Basic Utilities","text":"<pre><code>$ sudo apt install vim gedit-common\n$ sudo apt install ubuntu-restricted-extras\n$ sudo apt install vlc\n$ sudo apt install classicmenu-indicator\n$ sudo apt install redshift redshift-gtk\n$ sudo apt install terminator\n$ sudo apt install unity-tweak-tool\n</code></pre>","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#install-google-chrome-from-deb-package","title":"Install Google Chrome from .deb package","text":"","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#notes-and-others","title":"Notes and Others","text":"<pre><code>$ sudo apt install nixnote2 #(To connect with evernote)\n</code></pre>","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#install-simple-note-from-deb-package-take-notes-in-cross-platform-app","title":"Install Simple Note from .deb package (Take notes in cross platform app)","text":"","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#download-medium-desk-and-extract-it-to-opt-folder-read-blogs-on-medium","title":"Download Medium Desk and extract it to /opt folder (Read blogs on medium)","text":"","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#development-tools","title":"Development Tools","text":"<pre><code>$ sudo apt install gcc g++\n$ sudo apt install cmake build-essential\n$ sudo apt install python-pip python-dev (for python3 use: python3-pip , python3-dev\n$ sudo apt install openjdk-8-jre\n$ sudo apt install git\n</code></pre>","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#text-editors","title":"Text Editors","text":"<p>Install Sublime Text3 from official site</p> <p>Download Menlo font (Its my favorite font but it is not available in Ubuntu, so you have to install opensource version)</p>","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#install-vs-code-site","title":"Install VS Code site","text":"","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#python-for-data-science","title":"Python for Data Science","text":"","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#1-make-virtual-environment","title":"1. Make Virtual Environment","text":"<pre><code>$ sudo apt install python-virtualenv\n$ mkdir ~/env_name &amp;&amp; cd ~/env_name\n$ virtualenv env_name\n</code></pre>","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#2-activate-virtual-environment","title":"2. Activate virtual environment","text":"<pre><code>$ source ~/env_name/bin/activate\n</code></pre>","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#3-deactivate-virtual-environment","title":"3. Deactivate virtual environment","text":"<pre><code>$ deactivate\n</code></pre>","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#install-python-libraries","title":"Install Python Libraries","text":"<pre><code>$ pip install numpy matplotlib pandas\n$ pip install scipy scikit-learn scikit-image pillow\n$ pip install jupyter jupyterlab\n$ pip install nltk\n$ pip install spacy\n$ pip install tensorflow keras\n$ pip install xgboost\n</code></pre>","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#install-cuda","title":"Install CUDA","text":"","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#1-install-nvidia-graphics-driver-from-software-updates","title":"1. Install Nvidia Graphics Driver from Software &amp; Updates","text":"<p>Download cuda_x.deb packages  Download cudnn_y.deb packages </p> <pre><code>$ sudo dpkg -i cuda_x.deb\n$ sudo apt update\n$ sudo apt install cuda\n$ sudo apt upgrade\n</code></pre>","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#2-add-cuda-in-path","title":"2. Add Cuda in Path","text":"<p>Add following path to ~/.bashrc file</p> <pre><code>export CUDA_HOME=/usr/local/cuda-x.y\nexport LD_LIBRARY_PATH=${CUDA_HOME}/lib64\nPATH=${CUDA_HOME}/bin:${PATH}\nexport PATH\n</code></pre> <p>(where x and y are release numbers e.g. 7.5 or 8.0)</p> <p>Now we need to install Nvidia\u2019s library for deep learning CuDNN for which  you need to make developer\u2019s account in Nvidia. Download the required  cuddnn*.deb file.</p> <pre><code>$ sudo dpkg -i cudnn*.deb\n</code></pre>","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#install-deep-learning-frameworks","title":"Install Deep Learning Frameworks","text":"<pre><code>$ sudo apt install cuda-command-line-tools\n</code></pre>","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#add-following-lines-to-the-bashrc-file","title":"Add following lines to the ~/.bashrc file","text":"<pre><code>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64\n</code></pre>","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#install-tensorflow-gpu","title":"Install Tensorflow-GPU","text":"<pre><code>$ sudo pip install tensorflow-gpu keras\n</code></pre>","tags":["Ubuntu","Cuda"]},{"location":"blog/2019/08/15/setting-up-ubuntudebian-system-for-deep-learning/#install-pytorch-from-official-site","title":"Install Pytorch from official site","text":"","tags":["Ubuntu","Cuda"]},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2019/","title":"2019","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#cs","title":"CS","text":"<ul> <li>Free Machine Learning Courses</li> </ul>"},{"location":"tags/#cuda","title":"Cuda","text":"<ul> <li>Setting Up Ubuntu/Debian System for Deep Learning</li> </ul>"},{"location":"tags/#deep_learning","title":"Deep_Learning","text":"<ul> <li>Free Machine Learning Courses</li> </ul>"},{"location":"tags/#llm","title":"LLM","text":"<ul> <li>Brief Summary of Qwen Models</li> </ul>"},{"location":"tags/#llms","title":"LLMs","text":"<ul> <li>Top LLM Papers of 2024</li> </ul>"},{"location":"tags/#learning_resource","title":"Learning_Resource","text":"<ul> <li>Free Machine Learning Courses</li> </ul>"},{"location":"tags/#libraries","title":"Libraries","text":"<ul> <li>Useful Python Libraries for Machine Learning</li> <li>Phases of a Machine Learning Project</li> </ul>"},{"location":"tags/#machine_learning","title":"Machine_Learning","text":"<ul> <li>Free Machine Learning Courses</li> <li>Phases of a Machine Learning Project</li> </ul>"},{"location":"tags/#maths","title":"Maths","text":"<ul> <li>Free Machine Learning Courses</li> </ul>"},{"location":"tags/#nlp","title":"NLP","text":"<ul> <li>Keyword Extraction Methods</li> <li>Brief Summary of Qwen Models</li> </ul>"},{"location":"tags/#python","title":"Python","text":"<ul> <li>Useful Python Libraries for Machine Learning</li> <li>Phases of a Machine Learning Project</li> </ul>"},{"location":"tags/#research_papers","title":"Research_Papers","text":"<ul> <li>Top LLM Papers of 2024</li> <li>Brief Summary of Qwen Models</li> </ul>"},{"location":"tags/#ubuntu","title":"Ubuntu","text":"<ul> <li>Setting Up Ubuntu/Debian System for Deep Learning</li> </ul>"}]}